# Tier B â€“ Hosted generation, local embeddings
# Choose this when the team needs higher quality answers or longer replies.
# Questions go to Groq Llama 3.1 8B Instant while embeddings stay local so ingest
# costs remain zero. The frontend and routes stay the same.
MODEL_PROVIDER=groq
MODEL_NAME=phi3:mini  # kept for quick switch back to local
HOSTED_MODEL_NAME=llama-3.1-8b-instant
GROQ_API_KEY=YOUR_GROQ_KEY
GROQ_API_URL=https://api.groq.com/openai/v1/chat/completions
EMBED_PROVIDER=sentence

# Tip: unset MODEL_PROVIDER or switch it back to auto when you want local replies again.
