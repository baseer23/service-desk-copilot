# Tier A â€“ MacBook Air local default
# Use this template when running DeskMate on a MacBook Air. It prefers Phi 3 Mini
# for responsive local answers and automatically falls back to TinyLlama if you
# change MODEL_NAME. Embeddings stay on-device to keep ingest fast and private.
MODEL_PROVIDER=auto
MODEL_NAME=phi3:mini
EMBED_PROVIDER=sentence
HOSTED_MODEL_NAME=llama-3.1-8b-instant
GROQ_API_KEY=
GROQ_API_URL=https://api.groq.com/openai/v1/chat/completions

# Tip: if the laptop feels hot, switch to TinyLlama by setting MODEL_NAME=tinyllama
# and restarting the backend.
