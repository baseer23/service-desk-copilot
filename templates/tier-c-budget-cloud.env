# Tier C â€“ Budget cloud path with TinyLlama
# Deploy this on a low-cost cloud instance when the MacBook Air is offline.
# TinyLlama keeps resource usage low while embeddings continue to live locally.
MODEL_PROVIDER=ollama
MODEL_NAME=tinyllama
OLLAMA_HOST=http://localhost:11434
EMBED_PROVIDER=sentence
HOSTED_MODEL_NAME=llama-3.1-8b-instant
GROQ_API_KEY=
GROQ_API_URL=https://api.groq.com/openai/v1/chat/completions

# Tip: leave GROQ_API_KEY empty to avoid accidental hosted spend. Set it only
# when you plan to promote requests to Tier B.
